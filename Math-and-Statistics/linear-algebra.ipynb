{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <ins>**Linear Algebra**</ins>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importance to Data Science**\n",
    "Data is often represented in vectors and matrices. Linear algebra is the tool to handle and manipulate those. Linear algebra plays an important role in machine learning. For example one of the most simplest and common machine learning algorithm is *linear regression* which uses linear algebra to find best-fit line for predicting outcomes. It's also present in *optimization*, *neural networks*, *image recognition*, *recommendation systems* and many more areas. Knowing linear algebra is essential to computer and data science. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "## **Vectors**\n",
    "\n",
    "### <ins>What are Vectors?</ins>\n",
    "A vector is essentially an ordered list of numbers. They are used to represent data points, measurements or any kind of numeric information in a structured way. \n",
    "\n",
    "### <ins>Characteristics of Vectors</ins>\n",
    "\n",
    "#### 1. Dimension\n",
    "- The number of elements in a vector is called its dimension. For example a vector with 3 elements is called a 3-dimensional vector.\n",
    "\n",
    "#### 2. Notation\n",
    "- Vectors are often written as a column of numbers like this:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\begin{pmatrix} 1 \\\\ 2 \\\\ 3 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;or as a row of numbers:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = (1, 2, 3)\n",
    "$$\n",
    "\n",
    "#### 3. Components\n",
    "- Each number in the vector is called a component. For example in the vector $\\mathbf{v} = (1, 2, 3)$, the components are 1, 2 and 3.\n",
    "\n",
    "### <ins>Python Examples of Creating Vectors</ins>\n",
    "In Python we could use NumPy library for creating vectors. Creating a row vector is easy but the problem is that NumPy will treat column vectors like regular 1D arrays unless we explicitly shape it. This would make the column vector a 2D vector.\n",
    "\n",
    "#### Row Vector Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row vector: \n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "row_vector = np.array([1, 2, 3])\n",
    "print(f\"Row vector: \\n{row_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column Vector Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column vector: \n",
      "[[1]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "column_vector = np.array([[1], [2], [3]])\n",
    "print(f\"Column vector: \\n{column_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>How Vectors are Used in Data Science</ins>\n",
    "Here are some common use cases of vectors in data science:\n",
    "\n",
    "#### 1. Data Representation:\n",
    "- Features of a Data point: Each data point in a dataset can be represented as a vector. For example, if you are working with a dataset of bulking pandas where each panda is described by its height (in meters), weight (in kilograms) and age (in years), each panda can be represented as a 3-dimensional vector:\n",
    "\n",
    "$$\n",
    "Panda 1 = (1.0, 125.0, 8) \\\\\n",
    "Panda 2 = (1.1, 140.0, 15)\n",
    "$$\n",
    "\n",
    "#### 2. Operations on Vectors:\n",
    "- **Addition:** Vectors can be added together by adding their corresponding components. If $a = (1, 2)$ and $b = (3, 4)$, then: \n",
    "$$\n",
    "a + b = (1 + 3, 3 + 4) = (4, 6)\n",
    "$$\n",
    "\n",
    "- **Scalar Multiplication:** A vector can be multiplied by a scalar (a single number). You multiply each component with the scalar number. If $\\mathbf{v} = (2, 3)$ and the scalar is 4, then:\n",
    "$$\n",
    "4\\mathbf{v} = 4(2, 3) = (4 \\times{} 2, 4 \\times{} 3) = (8, 12)\n",
    "$$ \n",
    "\n",
    "#### 3. Distance and Similarity:\n",
    "- **Euclidean Distance:** The Euclidean distance between two vectors is a measure how far apart they are, the similarity between two data points. Often times distance and similarity are considered separate things. Distance tells you how far apart the vectors are while similarity tells you how similar or aligned the vectors are. Distance ranges from 0 to infinity while similarity can have negative metrics. Choosing which one to use depends on application. For example distance for clustering algorithms and similarity for information retrieval and text analysis. We are not going that deep here yet and focus on simpler things like Euclidean distance. For vectors $a = (x_1, y_1)$ and $b = (x_2, y_2)$, the distance is given by:\n",
    "$$\n",
    "Distance = \\sqrt{(x_2 - x_ 1)^2 + (y_2 - y_1)^2}\n",
    "$$\n",
    "\n",
    "#### 4. Direction and Magnitude:\n",
    "- **Direction:** The direction of the vector is the way it points in space. This is important in multiple disciplines like physics and engineering but also in understanding the orientation of data points in data science.\n",
    "\n",
    "- **Magnitude:** In this context magnitude means the measured length of the vector $\\mathbf{v} = (x, y)$ in space and is given by:\n",
    "$$\n",
    "||\\mathbf{v}|| = \\sqrt{x^2 + y^2}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;and more generally depending on number of dimensions for vector $\\mathbf{v} = (v_1, v_2...,v_n)$:\n",
    "\n",
    "$$\n",
    "||\\mathbf{v}|| = \\sqrt{v^2_1 + v^2_2 + ... + v^2_n}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Example: consider $\\mathbf{v} = (3, 4)$:\n",
    "\n",
    "$$\n",
    "||\\mathbf{v}|| = \\sqrt{3^2 + 4^4} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;This means the magnitude for the vector is 5.\n",
    "\n",
    "- **Further Understanding the Magnitude:** In context of vectors, the notation $||\\mathbf{v}||$ (read as *\"norm of v\"* or *\"magnitude of v\"*) is just a fancy way of saying magnitude. The double vertical bars $|| \\; ||$ are used to denote the magnitude of a vector. You do not have to go deeper interpreting it!\n",
    "\n",
    "### <ins>Python Examples of Vector Operations and Calculations</ins>\n",
    "For the sake of simplicity we are going to use functions in NumPy and not write everything from a scratch. This is how it would work in real life as well unless you need to build your own custom function. **Do not re-invent the wheel!** Here are few examples of manipulating vectors in python:\n",
    "\n",
    "#### Addition, Subtraction and Scalar Multiplication:\n",
    "I know there was nothing about subtraction above but it works the same as addition. Lets use the vectors above, $a = (1, 2)$ and $b = (3, 4)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition of vectors a and b is: [4 6]\n",
      "Subtraction of vectors a and b is: [-2 -2]\n"
     ]
    }
   ],
   "source": [
    "# Define the vectors\n",
    "a = np.array([1, 2])\n",
    "b = np.array([3, 4])\n",
    "\n",
    "# Addition\n",
    "vectors_added = a + b\n",
    "print(f\"Addition of vectors a and b is: {vectors_added}\")\n",
    "\n",
    "# Subtraction \n",
    "vectors_subtracted = a - b\n",
    "print(f\"Subtraction of vectors a and b is: {vectors_subtracted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next for scalar multiplication we use $\\mathbf{v} = (2, 3)$ with the scalar 4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar multiplication of vector is: [ 8 12]\n"
     ]
    }
   ],
   "source": [
    "# Define the vector and scalar\n",
    "v = np.array([2, 3])\n",
    "scalar = 4\n",
    "\n",
    "# Scalar multiplication\n",
    "vector_multiplied = v * scalar\n",
    "print(f\"Scalar multiplication of vector is: {vector_multiplied}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean Distance\n",
    "For this example lets use vectors $c = (1, 2, 3)$ and $d = (4, 5, 6)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean distance between c and d is: 5.196152422706632\n"
     ]
    }
   ],
   "source": [
    "# Define vectors\n",
    "c = np.array([1, 2, 3])\n",
    "d = np.array([4, 5, 6])\n",
    "\n",
    "# Calculate Euclidean distance\n",
    "euclidean_distance = np.linalg.norm(c - d)\n",
    "print(f\"Euclidean distance between c and d is: {euclidean_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Magnitude\n",
    "For magnitude lets use one of the vectors in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Magnitude for vector c is: 3.7416573867739413\n"
     ]
    }
   ],
   "source": [
    "magnitude = np.linalg.norm(c)\n",
    "print(f\"Magnitude for vector c is: {magnitude}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direction a.k.a. Normalization\n",
    "Now here is something new before the python code. Direction and normalization are the same in most cases. When we talk about the direction of the vector, we are often interested in *unit vector* that points the same direction as the original vector. Thus, the unit vector is obtained from normalizing the vector. The equation for normalization looks something like this: \n",
    "$$\n",
    "\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}} {||\\mathbf{v}||}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direction of the vector c is: [0.26726124 0.53452248 0.80178373]\n"
     ]
    }
   ],
   "source": [
    "# Using the c vector from above again\n",
    "direction = c / np.linalg.norm(c)\n",
    "print(f\"Direction of the vector c is: {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Final Notes About Vectors</ins>\n",
    "By final notes I mean one final example of vector use case in data science. Not going into details but giving this example as an extra. \n",
    "- Suppose that we have a dataset of athletic pandas who have scores (from 0 to 100) of two performances: Jumping and Running.\n",
    "\n",
    "$$\n",
    "Panda 1 = (85, 78) \\\\\n",
    "Panda 2 = (92, 88) \\\\\n",
    "Panda 3 = (45, 60) \\\\\n",
    "Panda 4 = (50, 65)\n",
    "$$\n",
    "\n",
    "- Each panda is represented as a simple 2-dimensional vector based on their scores.\n",
    "\n",
    "- We could use clustering algorithms like K-means clustering to group pandas with similar scores together. The algorithm calculates the distance between vectors from clusters.\n",
    "\n",
    "- The pandas might be clustered in two groups: high-performing and low-performing based on their vector scores. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "\n",
    "## **Matrices**\n",
    "\n",
    "### <ins>What is a Matrix?</ins>\n",
    "Other than a great sci-fi movie trilogy, a matrix is a rectangular array of numbers arranged in rows and columns. Each number in a matrix is called and element. Just like vectors, matrices are fundamental concept in linear algebra and are widely used in various fields - including data science - to represent and manipulate data. They are used various tasks such as data transformations and solving systems of linear equations. A matrix is typically denoted by a capital letter (e.g., $A, B, C$). Here is an example of a 3x3 matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \n",
    "\\begin{pmatrix}\n",
    "a_{1,1} & a_{1,2} & a_{1,3} \\\\\n",
    "a_{2,1} & a_{2,2} & a_{2,3} \\\\\n",
    "a_{3,1} & a_{3,2} & a_{3,3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### <ins>Basic Matrix Operations</ins>\n",
    "Matrices have similar operations to vectors but are done in rows and columns.\n",
    "\n",
    "#### 1. Addition and Subtraction:\n",
    "- You can add and subtract two or more matrices if they have the same dimensions. This means that for example you cannot add a 3x2 and 2x3 matrices together because they have different amount of rows and columns. The operation of addition as well as subtraction are performed element-wise.\n",
    "\n",
    "$$\n",
    "\\mathbf{A} + \\mathbf{B} =\n",
    "\\begin{pmatrix}\n",
    "a_{1,1} + b_{1,1} & a_{1,2} + b_{1,2}\\\\\n",
    "a_{2,1} + b_{2,1} & a_{2,2} + b_{2,2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Here is an example with something less abstract than letters:\n",
    "$$\n",
    "\\mathbf{A} =\n",
    "\\begin{pmatrix}\n",
    "1 & 2\\\\\n",
    "3 & 4\n",
    "\\end{pmatrix},\n",
    "\n",
    "\\mathbf{B} =\n",
    "\\begin{pmatrix}\n",
    "5 & 6\\\\\n",
    "7 & 8\n",
    "\\end{pmatrix},\n",
    "\n",
    "\\mathbf{A} + \\mathbf{B} =\n",
    "\\begin{pmatrix}\n",
    "1 + 5 & 2 + 6\\\\\n",
    "3 + 7 & 4 + 8\n",
    "\\end{pmatrix} =\n",
    "\n",
    "\\begin{pmatrix}\n",
    "6 & 8\\\\\n",
    "10 & 12\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### 2. Scalar Multiplication:\n",
    "- The scalar multiplication happens the same kind of way as seen with vectors. You have a single number we call a scalar and we multiply each element in the matrix with it.\n",
    "\n",
    "$$\n",
    "\\mathbf{c} \\cdot \\mathbf{A} =\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{c} \\cdot a_{1,1} & \\mathbf{c} \\cdot a_{1,2} \\\\\n",
    "\\mathbf{c} \\cdot a_{2,1} & \\mathbf{c} \\cdot a_{2,2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Lets multiply the $\\mathbf{A}$ matrix from above with the scalar $\\mathbf{c} = 4$:\n",
    "\n",
    "$$\n",
    "4 \\cdot \\mathbf{A} =\n",
    "\\begin{pmatrix}\n",
    "4 \\cdot 1 & 4 \\cdot 2 \\\\\n",
    "4 \\cdot 3 & 4 \\cdot 4\n",
    "\\end{pmatrix} = \n",
    "\n",
    "\\begin{pmatrix}\n",
    "4 & 8\\\\\n",
    "12 & 16\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### 3. Matrix Multiplication:\n",
    "- This one is a bit more tricky. To be able to multiply two matrices with each other, the first matrix must have the same number of columns as the second matrix has rows. The number of rows in the resulting matrix corresponds with the number of rows in the first matrix and the number of columns in the resulting matrix is the number of columns in the second matrix. So if we had matrixes 3x6 and 6x2, the resulting matrix would be in a form of 3x2. You can think of the 6 in 3x6 and 6x2 as some sort of \"same-number-link\" between the two. You would not be able to multiply 6x3 with 2x6. Here is the abstract of multiplying process:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} = \\mathbf{A} \\cdot \\mathbf{B} \\implies c_{ij} = \\displaystyle\\sum_{k} a_{ik} \\cdot b_{kj}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Next lets look at more concrete example for clarity:\n",
    "\n",
    "$$\n",
    "\\mathbf{A}\n",
    "\\begin{pmatrix}\n",
    "1 & 2 & 3\\\\\n",
    "4 & 5 & 6\n",
    "\\end{pmatrix},\n",
    "\n",
    "\\mathbf{B}\n",
    "\\begin{pmatrix}\n",
    "7 & 8\\\\\n",
    "9 & 10\\\\\n",
    "11 & 12\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;It would seem like the product matrix $\\mathbf{C}$ is a 2x2 matrix. Now the most tedious part:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**Calculate** $c_{1,1}$:\n",
    "\n",
    "$$\n",
    "c_{1,1} = a_{1,1} \\cdot b_{1,1} + a_{1,2} \\cdot b_{2,1} + a_{1,3} \\cdot b_{3,1}\\\\\n",
    "c_{1,1} = 1 \\cdot 7 + 2 \\cdot 9 + 3 \\cdot 11\\\\\n",
    "c_{1,1} = 7 + 18 + 33 = 58\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**Calculate** $c_{1,2}$:\n",
    "\n",
    "$$\n",
    "c_{1,2} = a_{1,1} \\cdot b_{1,2} + a_{1,2} \\cdot b_{2,2} + a_{1,3} \\cdot b_{3,2}\\\\\n",
    "c_{1,2} = 1 \\cdot 8 + 2 \\cdot 10 + 3 \\cdot 12\\\\\n",
    "c_{1,2} = 8 + 20 + 36 = 64\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**Calculate** $c_{2,1}$:\n",
    "\n",
    "$$\n",
    "c_{2,1} = a_{2,1} \\cdot b_{1,1} + a_{2,2} \\cdot b_{2,1} + a_{2,3} \\cdot b_{3,1}\\\\\n",
    "c_{2,1} = 4 \\cdot 7 + 5 \\cdot 9 + 6 \\cdot 11\\\\\n",
    "c_{2,1} = 28 + 45 + 66 = 139\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;**Calculate** $c_{2,2}$:\n",
    "\n",
    "$$\n",
    "c_{2,2} = a_{2,1} \\cdot b_{1,2} + a_{2,2} \\cdot b_{2,2} + a_{2,3} \\cdot b_{3,2}\\\\\n",
    "c_{2,2} = 4 \\cdot 8 + 5 \\cdot 10 + 6 \\cdot 12\\\\\n",
    "c_{2,2} = 32 + 50 + 72 = 154\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;This results to:\n",
    "\n",
    "$$\n",
    "\\mathbf{C} =\n",
    "\\begin{pmatrix}\n",
    "58 & 64\\\\\n",
    "139 & 154\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;As you can observe, this is quite a workload to do by hand. Luckily technology comes to the rescue and this can be done with few lines of Python code and NumPy. \n",
    "\n",
    "### <ins>Python Examples of Basic Matrix Operations</ins>\n",
    "Next we will see how easy it is to do matrix calculations in Python. \n",
    "\n",
    "#### 1. Matrix Addition and Subtraction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Addition: \n",
      "[[ 6  8]\n",
      " [10 12]]\n",
      "Matrix Subtraction: \n",
      "[[-4 -4]\n",
      " [-4 -4]]\n"
     ]
    }
   ],
   "source": [
    "# Define matrices\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "B = np.array([[5, 6], [7, 8]])\n",
    "\n",
    "# Addition\n",
    "addition_matrix = A + B\n",
    "print(f\"Matrix Addition: \\n{addition_matrix}\")\n",
    "\n",
    "# Subtraction\n",
    "subtraction_matrix = A - B\n",
    "print(f\"Matrix Subtraction: \\n{subtraction_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Matrix Scalar Multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar Multiplication: \n",
      "[[ 4  8]\n",
      " [12 16]]\n"
     ]
    }
   ],
   "source": [
    "# Define scalar\n",
    "scalar = 4\n",
    "scalar_matrix = scalar * A\n",
    "print(f\"Scalar Multiplication: \\n{scalar_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Matrix Multiplication:\n",
    "*TRIVIA:* Matrix multiplication is also called \"dot product\", thus the name of NumPy function ``dot()`` used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1 2 3]\n",
      " [4 5 6]]\n",
      "Matrix B:\n",
      " [[ 7  8]\n",
      " [ 9 10]\n",
      " [11 12]]\n",
      "Matrix C = (A * B):\n",
      " [[ 58  64]\n",
      " [139 154]]\n"
     ]
    }
   ],
   "source": [
    "# Redefine A and B to match what was taught above for clarity\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "B = np.array([[7, 8], [9, 10], [11, 12]])\n",
    "\n",
    "# Product matrix C\n",
    "C = np.dot(A, B)\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Matrix B:\\n\", B)\n",
    "print(\"Matrix C = (A * B):\\n\", C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Special Matrices</ins>\n",
    "There are couple matrices which are exceptional and for that reason important to know about. You will encounter them in more complex calculations on the road.  \n",
    "\n",
    "#### Identity Matrix:\n",
    "- Identity matrix is a sequence matrix with ones on diagonal and zeroes everywhere else. It is a square shape meaning it always has the same amount of rows and columns. It is commonly denoted with big $\\mathbf{I}$. For example:\n",
    "\n",
    "$$\n",
    "\\mathbf{I} =\n",
    "\\begin{pmatrix}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### Zero Matrix:\n",
    "- This matrix is exactly how it sounds like, full of zeros. You might also hear someone call it the \"null matrix\" or \"zero tensor\" which are both valid and latter is maybe heard more in context of Python machine learning libraries such as PyTorch. Unlike identity matrix, zero matrix can be of any size and is usually denoted with big letter $\\mathbf{O}$ (not zero). Here is a shocking example of 2x3 zero matrix you did not expect:\n",
    "\n",
    "$$\n",
    "\\mathbf{O} =\n",
    "\\begin{pmatrix}\n",
    "0 & 0 & 0\\\\\n",
    "0 & 0 & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "### <ins>Python Examples of Special Matrices</ins>\n",
    "Creating identity and zero matrices with NumPy is extremely easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identity Matrix: \n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n",
      "Zero Matrix: \n",
      "[[0. 0. 0.]\n",
      " [0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Identity matrix\n",
    "I = np.identity(3)\n",
    "print(f\"Identity Matrix: \\n{I}\")\n",
    "\n",
    "# Zero matrix\n",
    "O = np.zeros((2, 3))\n",
    "print(f\"Zero Matrix: \\n{O}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Matrix Properties</ins>\n",
    "\n",
    "#### 1. Transpose of a Matrix:\n",
    "The transpose of a matrix is when its rows and columns are swapped from $m \\times{} n$ to $n \\times{} m$. This is useful in many cases. For example if you remember the rule to multiply two matrices, we can transpose the other matrix in case multiplying was not possible otherwise. Say we have a matrix $\\mathbf{A}$ and we want to transpose it to $\\mathbf{A^T}$. This is how it would look like before and after:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} =\n",
    "\\begin{pmatrix}\n",
    "a_{1,2} & a_{1,2} & a_{1,3}\\\\\n",
    "a_{2,1} & a_{2,2} & a_{2,3}\n",
    "\\end{pmatrix} \\implies\n",
    "\n",
    "\\mathbf{A^T} =\n",
    "\\begin{pmatrix}\n",
    "a_{1,1} & a_{2,1}\\\\\n",
    "a_{1,2} & a_{2,2}\\\\\n",
    "a_{1,3} & a_{2,3}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "#### 2. Determinant of a Matrix:\n",
    "The determinant is a scalar value that can be computed from the elements of a square matrix. It is a useful value in linear algebra for various reasons, including determining whether a matrix is invertible and solving systems of linear equations. Here are properties of determinant:\n",
    "\n",
    "- **Invertibility**: A matrix is invertible if and only if its determinant is non-zero. \n",
    "\n",
    "- **Multiplicative Property**: The determinant of the product of two matrices is the product of their determinants. \n",
    "$$\n",
    "\\mathbf{det}(\\mathbf{A}\\mathbf{B}) = \\mathbf{det}(\\mathbf{A}) \\cdot \\mathbf{det}(\\mathbf{B})\n",
    "$$\n",
    "\n",
    "- **Transpose**: The determinant of a matrix is equal to the determinant of its transpose.\n",
    "\n",
    "$$\n",
    "\\mathbf{det}(\\mathbf{A^T}) = \\mathbf{det}(\\mathbf{A})\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Next is the calculation of the determinant. I will not share an example with numbers because it is a pain to do by hand. At this point you should be able to understand the abstract already.\n",
    "\n",
    "- **Determinant of a $2 \\times{} 2$ matrix**\n",
    "$$\n",
    "\\mathbf{A} =\n",
    "\\begin{pmatrix}\n",
    "a & b\\\\\n",
    "c & d\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- **The determinant is calculated as**:\n",
    "$$\n",
    "\\mathbf{det}(\\mathbf{A}) = ad - bc\n",
    "$$\n",
    "\n",
    "- **For $3 \\times{} 3$ it is slightly more complex but hopefully you are able to grasp the logic**:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} =\n",
    "\\begin{pmatrix}\n",
    "a & b & c\\\\\n",
    "d & e & f\\\\\n",
    "g & h & i\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- **The determinant is calculated using**:\n",
    "\n",
    "$$\n",
    "\\mathbf{det}(\\mathbf{A}) = a(ei - fh) - b(di - fg) + c(dh - eg)\n",
    "$$\n",
    "\n",
    "Going larger you will notice a pattern where $a$ is positive, $b$ is negative, $c$ is positive and $d$ would be negative again if it was a $4 \\times{} 4$ matrix. If this seems too difficult to understand, do not worry, there are other methods. For example a method called *Laplace expansion* might be easier to understand for some. Also note that you do not have to do these by hand probably ever. Later in Python examples you will see how simple it is to find the determinate using NumPy.\n",
    "\n",
    "#### 3. Inverse of a Matrix:\n",
    "The inverse matrix of a matrix $\\mathbf{A}$, denoted as $\\mathbf{A^{-1}}$, is such matrix that when multiplied by $\\mathbf{A}$ yields the identity matrix. Only square matrices have inverses and the matrix must be non-singular meaning it has a non-zero determinant. The inverse of matrix can be found by using the following formula:\n",
    "\n",
    "$$\n",
    "\\mathbf{A^{-1}} = \\frac{1} {\\mathbf{det(\\mathbf{A})}} \\mathbf{Adj(\\mathbf{A})}\n",
    "$$\n",
    "\n",
    "Now what is $\\mathbf{Adj(\\mathbf{A})}$? It means that we need to adjugate the matrix to correct the positions and weights of the elements to align them properly for inversion. For me it was hard to comprehend so I will be using a sort of metaphor to make it simpler to understand where the adjugate matrix comes from. For example:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} =\n",
    "\\begin{pmatrix}\n",
    "a & b\\\\\n",
    "c & d\n",
    "\\end{pmatrix} \\implies\n",
    "\n",
    "\\mathbf{Adj(\\mathbf{A})} =\n",
    "\\begin{pmatrix}\n",
    "d & -b\\\\\n",
    "-c & a\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "How did we get there? There is a 3-stage process. First we find *minors*, then we apply *cofactors* and lastly the matrix is transposed. Lets get to that metaphor I mentioned above. Imagine a house with 4 apartments and there lives a panda in each. The house is 2 stories tall so it resembles a $2 \\times{} 2$ matrix like above. All the pandas are very sensitive to sounds but also like to make noises themselves so they find their neighbors irritating. If we ask each panda how would they prefer to prefer to live, they would tell you that they do not want any wall neighbors. This means that the panda living in apartment $a$ for example would no not mind if panda in apartment $d$ was home or not but wants $b$ and $c$ gone. All pandas would give you a similar answer and if we make a matrix out of the neighbors they would not mind, it would look like this:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "d & c\\\\\n",
    "b & a\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This is what we call finding minors for each element, which sounds more mathematical than pandas wanting to live peacefully. Now the first stage is done and we can apply cofactors. Cofactor can be either positive or negative and follows the following checkerboard pattern regardless of how large the matrix is:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "+ & -\\\\\n",
    "- & +\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Applying to minors:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "d & -c\\\\\n",
    "-b & a\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now that the second stage is done, all that is left is to transpose the matrix the way that was taught earlier and we get the adjugate matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{Adj(\\mathbf{A})} =\n",
    "\\begin{pmatrix}\n",
    "d & -b\\\\\n",
    "-c & a\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Now you should have full understanding of how to find the inverse of matrix using the formula mentioned in the beginning of this sections. A common application of inverse matrix is to use it in solving systems of linear equations like:\n",
    "\n",
    "$$\n",
    "\\mathbf{Ax} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{A}$: is the coefficient matrix\n",
    "- $\\mathbf{x}$: is the vector of unknowns\n",
    "- $\\mathbf{b}$: is the vector of constants\n",
    "\n",
    "If $\\mathbf{A}$ is invertible, the solution can be found as:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{A^{-1}}\\mathbf{b}\n",
    "$$\n",
    "\n",
    "### <ins>Python Examples of Matrix Properties</ins>\n",
    "Now you get to see the beauty of simplicity and how little you have to write to solve some of these when you use NumPy.\n",
    "\n",
    "#### 1. Transpose Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose of A: \n",
      "[[1 4]\n",
      " [2 5]\n",
      " [3 6]]\n"
     ]
    }
   ],
   "source": [
    "# Define transpose matrix\n",
    "A = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "\n",
    "# Transpose the matrix\n",
    "A_transposed = np.transpose(A)\n",
    "print(f\"Transpose of A: \\n{A_transposed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Determinant of a Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant of A: -2.0000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Define a square matrix\n",
    "A = np.array([[1, 2], [3, 4]])\n",
    "\n",
    "# Calculate the determinant\n",
    "det_A = np.linalg.det(A)\n",
    "print(f\"Determinant of A: {det_A}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inverse of a Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse of A:\n",
      " [[-0.025  0.15 ]\n",
      " [ 0.175 -0.05 ]]\n"
     ]
    }
   ],
   "source": [
    "# Define a square matrix to inverse\n",
    "A = np.array([[2, 6], [7, 1]])\n",
    "\n",
    "# Inverse the matrix\n",
    "A_inverted = np.linalg.inv(A)\n",
    "print(\"Inverse of A:\\n\", A_inverted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <ins>Advanced Matrix Operations</ins>\n",
    "You can go way deeper in linear algebra than what is being discussed here, even with basics, but this notebook is suppose to summarize the basics of basics for understanding the fundamentals of data science. Regardless of basics, in this section we talk about couple more advanced matrix operations. \n",
    "\n",
    "#### Eigenvalues and Eigenvectors:\n",
    "These two are advanced yet fundamental concept in linear algebra, which are extensively used in data science and machine learning.\n",
    "\n",
    "- **Eigenvector**: A non-zero vector $\\mathbf{v}$ that when a linear transformation is applied to it, changes only in scale, not in direction. Mathematically for $\\mathbf{A}$, $\\mathbf{v}$ is an eigenvector if:\n",
    "\n",
    "$$\n",
    "\\mathbf{Av} = λ\\mathbf{v}\n",
    "$$\n",
    "\n",
    "where λ (pronounced lambda) is a scalar known as the eigenvalue corresponding to the eigenvector $\\mathbf{v}$.\n",
    "\n",
    "- **Eigenvalue**: A scalar λ such that there exists a non-zero vector $\\mathbf{v}$ (eigenvector) that satisfies the equation above.\n",
    "\n",
    "To explain these two simply, I will put this concept in \"panda-terms\". Imagine that we have a dance floor which is populated by dancing pandas. In this context the dancing pandas are eigenvectors. These pandas are also special in a way that they will always dance in the same direction, as they have a vector. When a scaling factor (either matrix $\\mathbf{A}$ or eigenvalue λ) is applied, the pandas might stretch, compress or change size but they wont change the direction. For example if eigenvalue is 2, the pandas will double in size but if the eigenvalue is 0.5, they will shrink half their length. \n",
    "\n",
    "***Special Note:*** $\\mathbf{A}$ does not equal λ. From the first look it might seem logical when you remove eigenvectors but $\\mathbf{A}$ is a matrix and λ is just a single scalar number or a factor.\n",
    "\n",
    "Knowing all this we can conclude that eigenvectors are just regular vectors but they have a special name when matrix transformation is applied. Eigenvalues are just special factors made to describe complex transformations. \n",
    "\n",
    "To find eigenvalues we need to use something called *characteristic equation* which looks like this:\n",
    "\n",
    "$$\n",
    "\\mathbf{det}(\\mathbf{A}-λ\\mathbf{I}) = 0\n",
    "$$\n",
    "\n",
    "For us to be able to subtract eigenvalue from out matrix, it needs to be made into one and this conveniently happens by multiplying it with identity matrix. Lets have an example, with numbers this time!\n",
    "\n",
    "Given the matrix:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} = \n",
    "\\begin{pmatrix}\n",
    "4 & 1\\\\\n",
    "2 & 3\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Applying characteristic equation:\n",
    "\n",
    "$$\n",
    "\\mathbf{det}\n",
    "\\begin{pmatrix}\n",
    "4-λ & 1\\\\\n",
    "2 & 3 - λ\n",
    "\\end{pmatrix} = 0\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;This simplifies to:\n",
    "\n",
    "$$\n",
    "(4 - λ)(3 - λ) - 2 \\cdot{} 1 = 0 \\implies λ^2 - 7λ + 10 = 0\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;Once we solve the quadratic equation, we get:\n",
    "\n",
    "$$\n",
    "λ = 5 \\ \\mathbf{or} \\ λ = 2\n",
    "$$\n",
    "\n",
    "Now that we know the eigenvalues, we can search for the eigenvectors, or dancing pandas if you are still thinking about them.\n",
    "\n",
    "For each eigenvalue we solve:\n",
    "\n",
    "$$\n",
    "(\\mathbf{A}-λ\\mathbf{I})\\mathbf{v} = 0\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For λ = 5 we get:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "-1 & 1\\\\\n",
    "2 & -2\n",
    "\\end{pmatrix}\\mathbf{v} = 0 \\implies\n",
    "\n",
    "\\mathbf{v_1} = \n",
    "\\begin{pmatrix}\n",
    "1\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;For λ = 2 we get:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "2 & 1\\\\\n",
    "2 & 1\n",
    "\\end{pmatrix}\\mathbf{v} = 0 \\implies\n",
    "\n",
    "\\mathbf{v_2} = \n",
    "\\begin{pmatrix}\n",
    "-1\\\\\n",
    "2\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Why are eigenvectors and eigenvalues important in data science? Here are few example applications:\n",
    "\n",
    "- **Principal Component Analysis (PCA)**: PCA is a dimensionality reduction technique that uses eigenvalues and eigenvectors. It transforms the data into a new coordinate system where the axes (principal components) are the directions of maximum variance. The eigenvectors of the covariance matrix of the data provide these directions, and the corresponding eigenvalues indicate the variance along these directions.\n",
    "\n",
    "- **Graph Theory**: In graph analysis, the adjacency matrix of a graph has eigenvalues and eigenvectors that provide insights into the properties of the graph. For example, the largest eigenvalue can give an idea about the connectivity of the graph.\n",
    "\n",
    "- **Stability Analysis**: In systems theory and control engineering, eigenvalues determine the stability of a system. If all eigenvalues of a system's matrix have negative real parts, the system is stable.\n",
    "\n",
    "- **Markov Chains**: The steady-state distribution of a Markov chain can be found using eigenvectors and eigenvalues. The transition matrix of the Markov chain has an eigenvector corresponding to the eigenvalue 1, which represents the steady-state distribution.\n",
    "\n",
    "\n",
    "#### Singular Value Decomposition (SVD):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
